{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv2d, Sequential, ModuleList, BatchNorm2d\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from PIL import Image\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import mobilenetv2_ssd_config\n",
    "\n",
    "# dataset dependencies\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "from torchvision import transforms, utils\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from FaceLandmarksDataset import *\n",
    "from ssd_like_model import *\n",
    "\n",
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointLoss(nn.Module):\n",
    "    def __init__(self, device, image_size = 300, anchors_dims=[\n",
    "                                                             [1/19, 1/19],\n",
    "                                                             [0.1, 0.1],\n",
    "                                                             [0.2, 0.2],\n",
    "                                                             [1/3, 1/3],\n",
    "                                                             [0.5, 0.5],\n",
    "                                                             [1.0, 1.0]\n",
    "                                                                ],\n",
    "                 lambda_noobj=0.5, lambda_coor=5, lambda_landmarks=2):\n",
    "        \"\"\"\n",
    "\n",
    "        :param anchors_dims: (list) of size num_of_anchor_box\n",
    "                                     containing [width, height]  anchors\n",
    "        :param lambda_noobj: from YOLO\n",
    "        :param lambda_coor: from YOLO\n",
    "        \"\"\"\n",
    "        super(JointLoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.anchors_dims = anchors_dims\n",
    "        self.mse_loss = nn.MSELoss(reduction='mean')\n",
    "        self.smooth_l1_loss = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.lambda_coor = lambda_coor\n",
    "        self.image_size = image_size\n",
    "        self.lambda_landmarks = lambda_landmarks\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        \"\"\"\n",
    "        Joint loss for landmarks(NME) and bounding boxes(localization loss, confidence loss)\n",
    "\n",
    "        :param prediction: (tuple) containing predictions for bounding boxes and landmarks\n",
    "        bounding boxes prediction:  torch.size(batch_size, grid_size, grid_size, num_anchor_box, 4+1)\n",
    "        landmarks prediction: torch.size(batch_size, num_anchor_box, 68, 2 or 3)\n",
    "\n",
    "        :param target: (tuple) containing target values  for bounding boxes and landmarks\n",
    "        bounding boxes targets: torch.size(batch_size, max_num_of_faces, 4+1)\n",
    "        landmarks targets: torch.size(batch_size, max_num_of_faces, 68, 2 or 3)\n",
    "\n",
    "        :return: normalized mean error(float), localization loss(float), confidence loss(float),\n",
    "        best predicted bbox matched with gt (torch.tensor)\n",
    "        \n",
    "        torch.Size([1, 3000, 136])  landmarks prediction: torch.size(batch_size, grid_size, grid_size, num_anchor_box, 68, 2 or 3)\n",
    "        torch.Size([1, 68, 2])  landmarks target SHAPE must be torch.size(batch_size, max_num_of_faces, 68, 2 or 3)\n",
    "        \"\"\"\n",
    "        bbox_prediction, landmarks_prediction = prediction\n",
    "        bbox_prediction, landmarks_prediction = bbox_prediction.to(self.device), landmarks_prediction.to(self.device)\n",
    "        gt_boxes, gt_conf, obj_mask, noobj_mask, gt_landmarks = self.get_mask(prediction, target)\n",
    "        # choose only landmarks which corresponds to cells with face in it\n",
    "        landmarks_pred = landmarks_prediction[obj_mask]\n",
    "        gt_landmarks = gt_landmarks[obj_mask]\n",
    "        # calculate Normalized Mean Error\n",
    "        nme = self.lambda_landmarks * self.nme(gt_landmarks, landmarks_pred, gt_boxes[:, :, :, :, 2:4][obj_mask])\n",
    "        # calculate localization error\n",
    "        bbox_pred = bbox_prediction[:, :, :, :, :4][obj_mask].to(self.device)\n",
    "        gt_boxes = gt_boxes[obj_mask]\n",
    "        loc_loss = self.lambda_coor * self.smooth_l1_loss(bbox_pred, gt_boxes)\n",
    "        # calculate confidence loss\n",
    "        # get conf mask where gt and where there is no gt\n",
    "        conf_pred = bbox_prediction[:, :, :, :, 4]  # torch.Size([batch_size, grid_size, grid_size, num_of_anchors])\n",
    "        conf_loss = self.lambda_noobj * self.mse_loss(conf_pred[noobj_mask],\n",
    "                                                      gt_conf[noobj_mask]) + self.mse_loss(\n",
    "            conf_pred[obj_mask], gt_conf[obj_mask])\n",
    "\n",
    "        return nme, loc_loss, conf_loss, self.non_maximum_suppression(bbox_prediction, landmarks_prediction)\n",
    "\n",
    "    def get_mask(self, prediction, target):\n",
    "        \"\"\"\n",
    "        Tool for calculating the loss\n",
    "        Calculates masks (filter then used as indexes) for prediction,\n",
    "         build target values with the same shape as prediction\n",
    "         and best predicted bboxes\n",
    "        :param prediction: (tuple) the same as in forward method\n",
    "        :param target: (tuple) the same as in forward method\n",
    "        :return: gt_boxes: torch.size(batch_size, grid_size, grid_size, num_anchors, 4),\n",
    "                 gt_conf: torch.size(batch_size, grid_size, grid_size, num_anchors),\n",
    "                 mask: torch.size(batch_size, grid_size, grid_size, num_anchors),\n",
    "                 conf_mask: torch.size(batch_size, grid_size, grid_size, num_anchors),\n",
    "                 gt_landmarks: torch.size(batch_size, grid_size, grid_size, num_anchors, 68, 2 or 3)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ground truth bbox - [c_x, c_y, w, h] all values in [0,1] w.r.t the whole image \n",
    "        anchor boxes - [0.5, 0.5, w, h] 0.5 w.r.t cell ; w,h in [0,1] w.r.t the whole image\n",
    "                        store all values w.r.t the whole image \n",
    "                        (transform 0.5 to value in [0,1] w.r.t. the whole image in the 'self._get_anchor_boxes)\n",
    "        predicted bbox - [ln(c_x), ln(c_y), ln(w), ln(h)] c_x, c_y w.r.t. cell w, h w.r.t. image\n",
    "\n",
    "        1) IoU between gt and anchor, gt -> anchor form: \n",
    "            [c_x, c_y, w, h] -> [c_x/9 - int(c_x/9), c_y - int(c_y/9), w, h]  \n",
    "      \n",
    "        \"\"\"\n",
    "        bbox_target, landmarks_target = target\n",
    "        bbox_prediction, landmarks_prediction = prediction\n",
    "\n",
    "        batch_size, a, num_anchors = bbox_prediction.size(0), bbox_prediction.size(1), bbox_prediction.size(2)\n",
    "\n",
    "        # to mark bboxes with high IoU between predicted and target\n",
    "        # in other words, mark (with 1) bbox with the face in it w.r.t. ground truth(gt)\n",
    "        obj_mask = torch.zeros(batch_size,a, num_anchors)\n",
    "        noobj_mask = torch.ones(batch_size,a, num_anchors)\n",
    "        # to store ground truth confidence scores and box coordinates\n",
    "        gt_conf = torch.zeros(batch_size,a, num_anchors)\n",
    "        gt_boxes = torch.zeros(batch_size, a, num_anchors, 4)\n",
    "        gt_landmarks = torch.zeros(batch_size, a, num_anchors, 68 * 2)\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            for target_idx in range(bbox_target.shape[1]):\n",
    "                # there is no target, continue\n",
    "                if bbox_target[batch_idx, target_idx].sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                # get ground truth box coordinates\n",
    "                gt_x = bbox_target[batch_idx, target_idx, 0]\n",
    "                gt_y = bbox_target[batch_idx, target_idx, 1]\n",
    "                gt_w = bbox_target[batch_idx, target_idx, 2]\n",
    "                gt_h = bbox_target[batch_idx, target_idx, 3]\n",
    "\n",
    "                # get grid box indices of ground truth box\n",
    "                # coordinates gt_x*grid_size and gt_y*grid_size w.r.t. cell size (one cell 1x1)\n",
    "                gt_i = int(gt_x * grid_size)\n",
    "                gt_j = int(gt_y * grid_size)\n",
    "                gt_box = torch.tensor([gt_x, gt_y, gt_w, gt_h]).unsqueeze(0).to(\n",
    "                    self.device)  # torch.size(0,4)\n",
    "                # get anchor box that has the highest iou with ground truth\n",
    "                anchor_boxes = self._get_anchor_boxes(gt_i, gt_j, grid_size)\n",
    "                anchor_iou = self._get_iou(gt_box, anchor_boxes)\n",
    "                # best matching anchor box\n",
    "                best_anchor_idx = torch.argmax(anchor_iou)\n",
    "\n",
    "                # mark best predicted box\n",
    "                obj_mask[batch_idx, gt_j, gt_i, best_anchor_idx] = 1\n",
    "                noobj_mask[batch_idx, gt_j, gt_i, best_anchor_idx] = 0\n",
    "\n",
    "                gt_conf[batch_idx, gt_j, gt_i, best_anchor_idx] = 1\n",
    "                gt_boxes[batch_idx, gt_j, gt_i, best_anchor_idx] = torch.log1p(gt_box)\n",
    "                gt_landmarks[batch_idx, gt_j, gt_i, best_anchor_idx] = landmarks_target[batch_idx, target_idx].view(\n",
    "                    68 * 2)\n",
    "\n",
    "        obj_mask = obj_mask.byte()  # to use then as indexes of tensor\n",
    "        noobj_mask = noobj_mask.byte()  # to use then as indexes of tensor\n",
    "\n",
    "        return gt_boxes.to(self.device), gt_conf.to(self.device), obj_mask, noobj_mask, gt_landmarks.to(\n",
    "            self.device)\n",
    "\n",
    "    def nme(self, gt_landmarks, pred_landmarks, boxes_shapes):\n",
    "        \"\"\"\n",
    "        Normalized mean error (NME) defined as the Euclidean distance\n",
    "        between the predicted and ground truth 2D landmarks averaged over\n",
    "        68 landmarks and normalized by the bounding box dimensions\n",
    "\n",
    "        :param gt_landmarks: torch.size(batch_size, 68*2)\n",
    "        :param pred_landmarks: torch.size(batch_size, 68*2)\n",
    "        :param boxes_shapes: [[width, height], ...]\n",
    "        :return: (float)\n",
    "        \"\"\"\n",
    "        nme = 0.0\n",
    "        batch_size = gt_landmarks.shape[0]\n",
    "        gt_landmarks = gt_landmarks.view(batch_size, 68, 2)\n",
    "        pred_landmarks = pred_landmarks.view(batch_size, 68, 2)\n",
    "        for batch_idx in range(batch_size):\n",
    "            sum = 0\n",
    "            for i in range(68):\n",
    "                euclidean_dist = torch.dist(gt_landmarks[batch_idx, i], pred_landmarks[batch_idx, i], 2)\n",
    "                sum += euclidean_dist\n",
    "            normalization_factor = math.sqrt(\n",
    "                boxes_shapes[batch_idx][0] * boxes_shapes[batch_idx][1])\n",
    "            nme += sum / (normalization_factor * 68 * batch_size)\n",
    "        return nme\n",
    "\n",
    "    def _get_iou(self, box1, box2):\n",
    "        \"\"\"\n",
    "        Calculates IoU for two tensors of bboxes\n",
    "        :param box1: torch.size(num_of_boxes_1, 4)\n",
    "        :param box2: torch.size(num_of_boxes_2, 4)\n",
    "        :return: torch.size(max(num_of_boxes_1, num_of_boxes_2), 4)\n",
    "        \"\"\"\n",
    "\n",
    "        b1 = self._format_bbox(box1)\n",
    "        b2 = self._format_bbox(box2)\n",
    "        b1_x1, b1_x2, b1_y1, b1_y2 = b1[:, 0], b1[:, 1], b1[:, 2], b1[:, 3]\n",
    "        b2_x1, b2_x2, b2_y1, b2_y2 = b2[:, 0], b2[:, 1], b2[:, 2], b2[:, 3]\n",
    "\n",
    "        intersect_x1 = torch.max(b1_x1, b2_x1)\n",
    "        intersect_y1 = torch.max(b1_y1, b2_y1)\n",
    "        intersect_x2 = torch.min(b1_x2, b2_x2)\n",
    "        intersect_y2 = torch.min(b1_y2, b2_y2)\n",
    "\n",
    "        intersect_area = (intersect_x2 - intersect_x1 + 1) * (intersect_y2 - intersect_y1 + 1)\n",
    "\n",
    "        # union area\n",
    "        b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "        b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "        iou = intersect_area / (b1_area + b2_area - intersect_area + 1e-16)\n",
    "        return iou\n",
    "\n",
    "    def _get_anchor_boxes(self, g_i, g_j, grid_size, center_x=0.5, center_y=0.5):\n",
    "        \"\"\"\n",
    "        Creates list of anchor boxes with given dimensions (height, width) w.r.t image size\n",
    "        anchor box =  [center_x, center_y, width, height] w.r.t. the whole image\n",
    "\n",
    "        :param center_x: x coordinate of the center of the anchor box w.r.t. cell\n",
    "        :param center_y: y coordinate of the center of the anchor box w.r.t. cell\n",
    "        :return: (tensor) torch.size(len(of anchor_aspect_ratios), 4)\n",
    "        \"\"\"\n",
    "\n",
    "        anchors = []\n",
    "        center_x = (center_x + g_i) / grid_size\n",
    "        center_y = (center_y + g_j) / grid_size\n",
    "        for dims in self.anchors_dims:\n",
    "            anchors.append([center_x, center_y, dims[0], dims[1]])\n",
    "        return torch.tensor(anchors).to(self.device)\n",
    "\n",
    "    def _format_bbox(self, box):\n",
    "        \"\"\"\n",
    "        Convert [[c_x, c_y, w, h], ...] to [[x_top_left_0, y_top_left_0, x_bottom_right_0, y_bottom_right_0], ...]\n",
    "        :param box: (torch.tensor) [[c_x, c_y, w, h], ...]\n",
    "        :return: (torch.tensor) [[x_top_left_0, y_top_left_0, x_bottom_right_0, y_bottom_right_0], ...]\n",
    "        \"\"\"\n",
    "\n",
    "        x1, x2 = (box[:, 0] - box[:, 2] / 2).unsqueeze(0), (\n",
    "                box[:, 0] + box[:, 2] / 2).unsqueeze(0)\n",
    "        y1, y2 = (box[:, 1] - box[:, 3] / 2).unsqueeze(0), (\n",
    "                box[:, 1] + box[:, 3] / 2).unsqueeze(0)\n",
    "        return torch.cat((torch.t(x1 * self.image_size), torch.t(x2 * self.image_size), torch.t(y1 * self.image_size),\n",
    "                          torch.t(y2 * self.image_size)), 1)\n",
    "\n",
    "    def non_maximum_suppression(self, bbox_prediction, landmarks_prediction, conf_thresh=0.5, iou_thresh=0.5):\n",
    "        batch_size, grid_size, num_anchors = bbox_prediction.size(0), bbox_prediction.size(1), bbox_prediction.size(3)\n",
    "\n",
    "        bbox_prediction = bbox_prediction.view(batch_size, grid_size * grid_size * num_anchors, 4 + 1)\n",
    "        bbox_prediction = torch.cat((torch.expm1(bbox_prediction[:, :, :4]), bbox_prediction[:, :, 4:]), dim=2)\n",
    "\n",
    "        landmarks_prediction = landmarks_prediction.view(batch_size, grid_size * grid_size * num_anchors, 68 * 2)\n",
    "\n",
    "        conf_mask = (bbox_prediction[:, :, 4] > conf_thresh).float().unsqueeze(2)\n",
    "        bbox_pred = bbox_prediction * conf_mask\n",
    "        landmarks_pred = landmarks_prediction * conf_mask\n",
    "\n",
    "        bbox = []\n",
    "        landmarks = []\n",
    "        for i in range(batch_size):\n",
    "            image_bbox = bbox_pred[i]\n",
    "            image_landmarks = landmarks_pred[i]\n",
    "            max_conf_idx = torch.argmax(image_bbox[:, 4])\n",
    "            ious = self._get_iou(image_bbox[max_conf_idx].unsqueeze(0), image_bbox)\n",
    "            mask = ious < iou_thresh\n",
    "            bbox.append(image_bbox[mask.byte()])\n",
    "            bbox.append(image_bbox[max_conf_idx].unsqueeze(0))\n",
    "            landmarks.append(image_landmarks[mask.byte()])\n",
    "            landmarks.append(image_landmarks[max_conf_idx].unsqueeze(0))\n",
    "        bbox = torch.cat(bbox, dim=0)\n",
    "        bbox = torch.cat((self._format_bbox(bbox), bbox[:, 4:]), dim=1)\n",
    "        landmarks = torch.cat(landmarks, dim=0)\n",
    "        return bbox, landmarks.view(-1, 68, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(1):\n",
    "#     batch = next(iter(train_loader))\n",
    "#     print(len(batch))\n",
    "#     print(batch[0].shape)   #INPUT image\n",
    "#     print(batch[1][0][0].shape) # Boxes \n",
    "#     print(batch[1][0][1].shape) # Coordintes of landmarks\n",
    "#     condifence, locations,landmarks = net(batch[0])\n",
    "\n",
    "# images, boxes, labels = data\n",
    "# images = images.to(device)\n",
    "# boxes = boxes.to(device)\n",
    "# labels = labels.to(device)\n",
    "def train(loader, net, criterion, optimizer, device, debug_steps=100, epoch=-1):\n",
    "    net.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_regression_loss = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "            \n",
    "    for i, data in enumerate(loader):\n",
    "        images, gt_boxes = data\n",
    "        print(len(gt_boxes), type(gt_boxes[0]), \" Boxes len\")\n",
    "        images = images.to(device)\n",
    "        gt_locations = gt_boxes[0][0].to(device)\n",
    "        gt_landmarks = gt_boxes[0][1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        confidence, locations, landmarks_prediction = net(images)\n",
    "        \"\"\"\n",
    "            Joint loss for landmarks(NME) and bounding boxes(localization loss, confidence loss)\n",
    "            :param prediction: (tuple) containing predictions for bounding boxes and landmarks\n",
    "            bounding boxes prediction:  torch.size(batch_size, grid_size, grid_size, num_anchor_box, 4+1)\n",
    "            landmarks prediction: torch.size(batch_size, grid_size, grid_size, num_anchor_box, 68, 2 or 3)\n",
    "\n",
    "            :param target: (tuple) containing target values  for bounding boxes and landmarks\n",
    "            bounding boxes targets: torch.size(batch_size, max_num_of_faces, 4+1)\n",
    "            landmarks targets: torch.size(batch_size, max_num_of_faces, 68, 2 or 3)\n",
    "\n",
    "            :return: normalized mean error(float), localization loss(float), confidence loss(float),\n",
    "            best predicted bbox matched with gt (torch.tensor)\n",
    "        \"\"\"\n",
    "        \n",
    "        landmarks_prediction = landmarks_prediction.resize_((1,len(landmarks_prediction[0]),68,2))\n",
    "        print(landmarks_prediction.shape, \" landmarks prediction: torch.size(batch_size, grid_size, grid_size, num_anchor_box, 68, 2 or 3)\")\n",
    "        print(gt_landmarks.shape, \" landmarks target SHAPE must be torch.size(batch_size, max_num_of_faces, 68, 2 or 3)\")\n",
    "        print()\n",
    "        bounding_boxes_prediction = torch.cat( (locations, condifence), dim=2)\n",
    "        \n",
    "        prediction = (bounding_boxes_prediction, landmarks_prediction)\n",
    "        target = (gt_locations, gt_landmarks)\n",
    "        #TODO convert locations to bounding box predictions\n",
    "        nme, loc_loss, conf_loss, bbox_confirmed, landmarks_confirmed = criterion( prediction,  )  # TODO CHANGE BOXES\n",
    "        #Output from joint loss format\n",
    "        #nme, loc_loss, conf_loss, self.non_maximum_suppression(bbox_prediction, landmarks_prediction)\n",
    "        loss = nme + loc_loss + conf_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_nme_loss += nme.item()\n",
    "        running_loc_loss += loc_loss.item()\n",
    "        running_conf_loc_loss += conf_loss.item()\n",
    "        if i and i % debug_steps == 0:\n",
    "            avg_loss = running_loss / debug_steps\n",
    "            avg_nme_loss = running_nme_loss / debug_steps\n",
    "            avg_loc_loss = running_loc_loss / debug_steps\n",
    "            avg_conf_loc_loss = running_conf_loc_loss / debug_steps\n",
    "            logging.info(\n",
    "                f\"Epoch: {epoch}, Step: {i}, \" +\n",
    "                f\"Average Loss: {avg_loss:.4f}, \" +\n",
    "                f\"Average nme Loss {avg_nme_loss:.4f}, \" +\n",
    "                f\"Average loc Loss: {avg_loc_loss:.4f}\" +\n",
    "                f\"Average conf Loss: {avg_conf_loc_loss:.4f}\"\n",
    "            )\n",
    "            running_loss = 0\n",
    "            running_nme_loss = 0\n",
    "            running_loc_loss = 0\n",
    "            running_conf_loc_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, net, criterion, device):\n",
    "    net.eval()\n",
    "    running_loss = 0.0\n",
    "    running_regression_loss = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    num = 0\n",
    "    for _, data in enumerate(loader):\n",
    "        images, boxes, labels = data\n",
    "        images = images.to(device)\n",
    "        boxes = boxes.to(device)\n",
    "        labels = labels.to(device)\n",
    "        num += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            confidence, locations = net(images)\n",
    "            regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)\n",
    "            loss = regression_loss + classification_loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_regression_loss += regression_loss.item()\n",
    "        running_classification_loss += classification_loss.item()\n",
    "    return running_loss / num, running_regression_loss / num, running_classification_loss / num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-17 22:15:29,154 - root - INFO - Use Cuda.\n"
     ]
    }
   ],
   "source": [
    "scheduler = \"cosine\"\n",
    "lr = 0.01\n",
    "t_max = 200 # Params for Cosine Annealing\n",
    "\n",
    "mb2_width_mult = 1.0 #Width Multiplifier for MobilenetV2\n",
    "# Params for SGD\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1\n",
    "\n",
    "# Params for Multi-step Scheduler\n",
    "milestones = \"80,100\" #milestones for MultiStepLR\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "num_workers = 0\n",
    "validation_epochs = 0 \n",
    "debug_steps = 100 #Set the debug log output frequency\n",
    "\n",
    "# Train params\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    logging.info(\"Use Cuda.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphPath = namedtuple(\"GraphPath\", ['s0', 'name', 's1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-17 22:15:29,176 - root - INFO - Prepare training datasets.\n",
      "2019-11-17 22:15:29,265 - root - INFO - Train dataset size: 3449\n"
     ]
    }
   ],
   "source": [
    "config = mobilenetv2_ssd_config\n",
    "\n",
    "logging.info(\"Prepare training datasets.\")\n",
    "\n",
    "# PREPARE DATASET\n",
    "batch_size = 1\n",
    "test_labels = pd.read_csv('./LS3D-W/test.csv')\n",
    "train_dataset = FaceLandmarksDataset(test_labels)\n",
    "train_loader = DataLoader (train_dataset, batch_size = batch_size, shuffle = True, pin_memory = True)\n",
    "logging.info(\"Train dataset size: {}\".format(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network created\n"
     ]
    }
   ],
   "source": [
    "net = CustomSSD()\n",
    "min_loss = -10000.0\n",
    "last_epoch = -1\n",
    "#net.init_from_base_net(base_net)\n",
    "net.to(DEVICE)\n",
    "print(\"Network created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1, 3, 300, 300])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 68, 2])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    batch = next(iter(train_loader))\n",
    "    print(len(batch))\n",
    "    print(batch[0].shape)   #INPUT image\n",
    "    print(batch[1][0][0].shape) # Boxes \n",
    "    print(batch[1][0][1].shape) # Coordinates of landmarks\n",
    "    condifence, locations,landmarks = net(batch[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-17 22:15:30,950 - root - INFO - Uses CosineAnnealingLR scheduler.\n",
      "2019-11-17 22:15:30,952 - root - INFO - Start training from epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\Installed\\Anaconda\\envs\\keras-gpu\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <class 'list'>  Boxes len\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot resize variables that require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9f5263a7ed85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     train(train_loader, net, criterion, optimizer,\n\u001b[1;32m---> 28\u001b[1;33m           device=DEVICE, debug_steps=debug_steps, epoch=epoch)\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalidation_epochs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-880173760366>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loader, net, criterion, optimizer, device, debug_steps, epoch)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \"\"\"\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mlandmarks_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlandmarks_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlandmarks_prediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m68\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlandmarks_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" landmarks prediction: torch.size(batch_size, grid_size, grid_size, num_anchor_box, 68, 2 or 3)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt_landmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" landmarks target SHAPE must be torch.size(batch_size, max_num_of_faces, 68, 2 or 3)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cannot resize variables that require grad"
     ]
    }
   ],
   "source": [
    "#Заменить на своё\n",
    "criterion = JointLoss(device = DEVICE)\n",
    "\n",
    "params = [\n",
    "    {'params': net.base.parameters(), 'lr': lr},\n",
    "    {'params': itertools.chain(\n",
    "        net.source_layer_add_ons.parameters(),\n",
    "        net.extras.parameters()\n",
    "    ), 'lr': lr},\n",
    "    {'params': itertools.chain(\n",
    "        net.regression_headers.parameters(),\n",
    "        net.classification_headers.parameters(),\n",
    "        net.landmarks_headers.parameters()\n",
    "    )}\n",
    "]   \n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(params, lr = lr, momentum = momentum,\n",
    "                            weight_decay = weight_decay)\n",
    "\n",
    "#args.scheduler == 'cosine':\n",
    "logging.info(\"Uses CosineAnnealingLR scheduler.\")\n",
    "scheduler = CosineAnnealingLR(optimizer, t_max, last_epoch=last_epoch)\n",
    "\n",
    "logging.info(f\"Start training from epoch {last_epoch + 1}.\")\n",
    "for epoch in range(last_epoch + 1, num_epochs):\n",
    "    scheduler.step()\n",
    "    train(train_loader, net, criterion, optimizer,\n",
    "          device=DEVICE, debug_steps=debug_steps, epoch=epoch)\n",
    "\n",
    "    if epoch % validation_epochs == 0 or epoch == num_epochs - 1:\n",
    "        val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)\n",
    "        logging.info(\n",
    "            f\"Epoch: {epoch}, \" +\n",
    "            f\"Validation Loss: {val_loss:.4f}, \" +\n",
    "            f\"Validation Regression Loss {val_regression_loss:.4f}, \" +\n",
    "            f\"Validation Classification Loss: {val_classification_loss:.4f}\"\n",
    "        )\n",
    "        model_path = os.path.join(checkpoint_folder, f\"{net}-Epoch-{epoch}-Loss-{val_loss}.pth\")\n",
    "        net.save(model_path)\n",
    "        logging.info(f\"Saved model {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
