{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback available to show.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv2d, Sequential, ModuleList, BatchNorm2d\n",
    "from torchvision import transforms\n",
    "import math\n",
    "from PIL import Image\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# VISION\n",
    "from vision.datasets.voc_dataset import VOCDataset\n",
    "from vision.utils.misc import str2bool, Timer, freeze_net_layers, store_labels\n",
    "from vision.ssd.ssd import MatchPrior\n",
    "from vision.nn.multibox_loss import MultiboxLoss\n",
    "from vision.ssd.config import mobilenetv1_ssd_config\n",
    "from vision.ssd.data_preprocessing import TrainAugmentation, TestTransform\n",
    "\n",
    "# dataset dependencies\n",
    "from PIL import Image\n",
    "from torchvision import transforms, utils\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SeperableConv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "    return Sequential(\n",
    "        Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,\n",
    "               groups=in_channels, stride=stride, padding=padding),\n",
    "        BatchNorm2d(in_channels),\n",
    "        nn.ReLU6(),\n",
    "        Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n",
    "    )\n",
    "\n",
    "def conv_bn(inp, oup, stride, use_batch_norm=True):\n",
    "    if use_batch_norm:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup, use_batch_norm=True):\n",
    "    if use_batch_norm:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "            nn.ReLU6(inplace=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, use_batch_norm=True):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            if use_batch_norm:\n",
    "                self.conv = nn.Sequential(\n",
    "                    # dw\n",
    "                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                    nn.BatchNorm2d(hidden_dim),\n",
    "                    nn.ReLU6(inplace=True),\n",
    "                    # pw-linear\n",
    "                    nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                    nn.BatchNorm2d(oup),\n",
    "                )\n",
    "            else:\n",
    "                self.conv = nn.Sequential(\n",
    "                    # dw\n",
    "                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                    nn.ReLU6(inplace=True),\n",
    "                    # pw-linear\n",
    "                    nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                )\n",
    "        else:\n",
    "            if use_batch_norm:\n",
    "                self.conv = nn.Sequential(\n",
    "                    # pw\n",
    "                    nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                    nn.BatchNorm2d(hidden_dim),\n",
    "                    nn.ReLU6(inplace=True),\n",
    "                    # dw\n",
    "                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                    nn.BatchNorm2d(hidden_dim),\n",
    "                    nn.ReLU6(inplace=True),\n",
    "                    # pw-linear\n",
    "                    nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                    nn.BatchNorm2d(oup),\n",
    "                )\n",
    "            else:\n",
    "                self.conv = nn.Sequential(\n",
    "                    # pw\n",
    "                    nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                    nn.ReLU6(inplace=True),\n",
    "                    # dw\n",
    "                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                    nn.ReLU6(inplace=True),\n",
    "                    # pw-linear\n",
    "                    nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1., dropout_ratio=0.2,\n",
    "                 use_batch_norm=True):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s,\n",
    "                                               expand_ratio=t, use_batch_norm=use_batch_norm))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1,\n",
    "                                               expand_ratio=t, use_batch_norm=use_batch_norm))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel,\n",
    "                                         use_batch_norm=use_batch_norm))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_ratio),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointLoss(nn.Module):\n",
    "    def __init__(self, device, image_size = 300, anchors_dims=[\n",
    "                                                             [1/19, 1/19],\n",
    "                                                             [0.1, 0.1],\n",
    "                                                             [0.2, 0.2],\n",
    "                                                             [1/3, 1/3],\n",
    "                                                             [0.5, 0.5],\n",
    "                                                             [1.0, 1.0]\n",
    "                                                                ],\n",
    "                 lambda_noobj=0.5, lambda_coor=5, lambda_landmarks=2):\n",
    "        \"\"\"\n",
    "\n",
    "        :param anchors_dims: (list) of size num_of_anchor_box\n",
    "                                     containing [width, height]  anchors\n",
    "        :param lambda_noobj: from YOLO\n",
    "        :param lambda_coor: from YOLO\n",
    "        \"\"\"\n",
    "        super(JointLoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.anchors_dims = anchors_dims\n",
    "        self.mse_loss = nn.MSELoss(reduction='mean')\n",
    "        self.smooth_l1_loss = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.lambda_coor = lambda_coor\n",
    "        self.image_size = image_size\n",
    "        self.lambda_landmarks = lambda_landmarks\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        \"\"\"\n",
    "        Joint loss for landmarks(NME) and bounding boxes(localization loss, confidence loss)\n",
    "\n",
    "        :param prediction: (tuple) containing predictions for bounding boxes and landmarks\n",
    "        bounding boxes prediction:  torch.size(batch_size, grid_size, grid_size, num_anchor_box, 4+1)\n",
    "        landmarks prediction: torch.size(batch_size, num_anchor_box, 68, 2 or 3)\n",
    "\n",
    "        :param target: (tuple) containing target values  for bounding boxes and landmarks\n",
    "        bounding boxes targets: torch.size(batch_size, max_num_of_faces, 4+1)\n",
    "        landmarks targets: torch.size(batch_size, max_num_of_faces, 68, 2 or 3)\n",
    "\n",
    "        :return: normalized mean error(float), localization loss(float), confidence loss(float),\n",
    "        best predicted bbox matched with gt (torch.tensor)\n",
    "        \n",
    "        torch.Size([1, 3000, 136])  landmarks prediction: torch.size(batch_size, grid_size, grid_size, num_anchor_box, 68, 2 or 3)\n",
    "        torch.Size([1, 68, 2])  landmarks target SHAPE must be torch.size(batch_size, max_num_of_faces, 68, 2 or 3)\n",
    "        \"\"\"\n",
    "        bbox_prediction, landmarks_prediction = prediction\n",
    "        bbox_prediction, landmarks_prediction = bbox_prediction.to(self.device), landmarks_prediction.to(self.device)\n",
    "        gt_boxes, gt_conf, obj_mask, noobj_mask, gt_landmarks = self.get_mask(prediction, target)\n",
    "        # choose only landmarks which corresponds to cells with face in it\n",
    "        landmarks_pred = landmarks_prediction[obj_mask]\n",
    "        gt_landmarks = gt_landmarks[obj_mask]\n",
    "        # calculate Normalized Mean Error\n",
    "        nme = self.lambda_landmarks * self.nme(gt_landmarks, landmarks_pred, gt_boxes[:, :, :, :, 2:4][obj_mask])\n",
    "        # calculate localization error\n",
    "        bbox_pred = bbox_prediction[:, :, :, :, :4][obj_mask].to(self.device)\n",
    "        gt_boxes = gt_boxes[obj_mask]\n",
    "        loc_loss = self.lambda_coor * self.smooth_l1_loss(bbox_pred, gt_boxes)\n",
    "        # calculate confidence loss\n",
    "        # get conf mask where gt and where there is no gt\n",
    "        conf_pred = bbox_prediction[:, :, :, :, 4]  # torch.Size([batch_size, grid_size, grid_size, num_of_anchors])\n",
    "        conf_loss = self.lambda_noobj * self.mse_loss(conf_pred[noobj_mask],\n",
    "                                                      gt_conf[noobj_mask]) + self.mse_loss(\n",
    "            conf_pred[obj_mask], gt_conf[obj_mask])\n",
    "\n",
    "        return nme, loc_loss, conf_loss, self.non_maximum_suppression(bbox_prediction, landmarks_prediction)\n",
    "\n",
    "    def get_mask(self, prediction, target):\n",
    "        \"\"\"\n",
    "        Tool for calculating the loss\n",
    "        Calculates masks (filter then used as indexes) for prediction,\n",
    "         build target values with the same shape as prediction\n",
    "         and best predicted bboxes\n",
    "        :param prediction: (tuple) the same as in forward method\n",
    "        :param target: (tuple) the same as in forward method\n",
    "        :return: gt_boxes: torch.size(batch_size, grid_size, grid_size, num_anchors, 4),\n",
    "                 gt_conf: torch.size(batch_size, grid_size, grid_size, num_anchors),\n",
    "                 mask: torch.size(batch_size, grid_size, grid_size, num_anchors),\n",
    "                 conf_mask: torch.size(batch_size, grid_size, grid_size, num_anchors),\n",
    "                 gt_landmarks: torch.size(batch_size, grid_size, grid_size, num_anchors, 68, 2 or 3)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ground truth bbox - [c_x, c_y, w, h] all values in [0,1] w.r.t the whole image \n",
    "        anchor boxes - [0.5, 0.5, w, h] 0.5 w.r.t cell ; w,h in [0,1] w.r.t the whole image\n",
    "                        store all values w.r.t the whole image \n",
    "                        (transform 0.5 to value in [0,1] w.r.t. the whole image in the 'self._get_anchor_boxes)\n",
    "        predicted bbox - [ln(c_x), ln(c_y), ln(w), ln(h)] c_x, c_y w.r.t. cell w, h w.r.t. image\n",
    "\n",
    "        1) IoU between gt and anchor, gt -> anchor form: \n",
    "            [c_x, c_y, w, h] -> [c_x/9 - int(c_x/9), c_y - int(c_y/9), w, h]  \n",
    "      \n",
    "        \"\"\"\n",
    "        bbox_target, landmarks_target = target\n",
    "        bbox_prediction, landmarks_prediction = prediction\n",
    "\n",
    "        batch_size, a, num_anchors = bbox_prediction.size(0), bbox_prediction.size(1), bbox_prediction.size(2)\n",
    "\n",
    "        # to mark bboxes with high IoU between predicted and target\n",
    "        # in other words, mark (with 1) bbox with the face in it w.r.t. ground truth(gt)\n",
    "        obj_mask = torch.zeros(batch_size,a, num_anchors)\n",
    "        noobj_mask = torch.ones(batch_size,a, num_anchors)\n",
    "        # to store ground truth confidence scores and box coordinates\n",
    "        gt_conf = torch.zeros(batch_size,a, num_anchors)\n",
    "        gt_boxes = torch.zeros(batch_size, a, num_anchors, 4)\n",
    "        gt_landmarks = torch.zeros(batch_size, a, num_anchors, 68 * 2)\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            for target_idx in range(bbox_target.shape[1]):\n",
    "                # there is no target, continue\n",
    "                if bbox_target[batch_idx, target_idx].sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                # get ground truth box coordinates\n",
    "                gt_x = bbox_target[batch_idx, target_idx, 0]\n",
    "                gt_y = bbox_target[batch_idx, target_idx, 1]\n",
    "                gt_w = bbox_target[batch_idx, target_idx, 2]\n",
    "                gt_h = bbox_target[batch_idx, target_idx, 3]\n",
    "\n",
    "                # get grid box indices of ground truth box\n",
    "                # coordinates gt_x*grid_size and gt_y*grid_size w.r.t. cell size (one cell 1x1)\n",
    "                gt_i = int(gt_x * grid_size)\n",
    "                gt_j = int(gt_y * grid_size)\n",
    "                gt_box = torch.tensor([gt_x, gt_y, gt_w, gt_h]).unsqueeze(0).to(\n",
    "                    self.device)  # torch.size(0,4)\n",
    "                # get anchor box that has the highest iou with ground truth\n",
    "                anchor_boxes = self._get_anchor_boxes(gt_i, gt_j, grid_size)\n",
    "                anchor_iou = self._get_iou(gt_box, anchor_boxes)\n",
    "                # best matching anchor box\n",
    "                best_anchor_idx = torch.argmax(anchor_iou)\n",
    "\n",
    "                # mark best predicted box\n",
    "                obj_mask[batch_idx, gt_j, gt_i, best_anchor_idx] = 1\n",
    "                noobj_mask[batch_idx, gt_j, gt_i, best_anchor_idx] = 0\n",
    "\n",
    "                gt_conf[batch_idx, gt_j, gt_i, best_anchor_idx] = 1\n",
    "                gt_boxes[batch_idx, gt_j, gt_i, best_anchor_idx] = torch.log1p(gt_box)\n",
    "                gt_landmarks[batch_idx, gt_j, gt_i, best_anchor_idx] = landmarks_target[batch_idx, target_idx].view(\n",
    "                    68 * 2)\n",
    "\n",
    "        obj_mask = obj_mask.byte()  # to use then as indexes of tensor\n",
    "        noobj_mask = noobj_mask.byte()  # to use then as indexes of tensor\n",
    "\n",
    "        return gt_boxes.to(self.device), gt_conf.to(self.device), obj_mask, noobj_mask, gt_landmarks.to(\n",
    "            self.device)\n",
    "\n",
    "    def nme(self, gt_landmarks, pred_landmarks, boxes_shapes):\n",
    "        \"\"\"\n",
    "        Normalized mean error (NME) defined as the Euclidean distance\n",
    "        between the predicted and ground truth 2D landmarks averaged over\n",
    "        68 landmarks and normalized by the bounding box dimensions\n",
    "\n",
    "        :param gt_landmarks: torch.size(batch_size, 68*2)\n",
    "        :param pred_landmarks: torch.size(batch_size, 68*2)\n",
    "        :param boxes_shapes: [[width, height], ...]\n",
    "        :return: (float)\n",
    "        \"\"\"\n",
    "        nme = 0.0\n",
    "        batch_size = gt_landmarks.shape[0]\n",
    "        gt_landmarks = gt_landmarks.view(batch_size, 68, 2)\n",
    "        pred_landmarks = pred_landmarks.view(batch_size, 68, 2)\n",
    "        for batch_idx in range(batch_size):\n",
    "            sum = 0\n",
    "            for i in range(68):\n",
    "                euclidean_dist = torch.dist(gt_landmarks[batch_idx, i], pred_landmarks[batch_idx, i], 2)\n",
    "                sum += euclidean_dist\n",
    "            normalization_factor = math.sqrt(\n",
    "                boxes_shapes[batch_idx][0] * boxes_shapes[batch_idx][1])\n",
    "            nme += sum / (normalization_factor * 68 * batch_size)\n",
    "        return nme\n",
    "\n",
    "    def _get_iou(self, box1, box2):\n",
    "        \"\"\"\n",
    "        Calculates IoU for two tensors of bboxes\n",
    "        :param box1: torch.size(num_of_boxes_1, 4)\n",
    "        :param box2: torch.size(num_of_boxes_2, 4)\n",
    "        :return: torch.size(max(num_of_boxes_1, num_of_boxes_2), 4)\n",
    "        \"\"\"\n",
    "\n",
    "        b1 = self._format_bbox(box1)\n",
    "        b2 = self._format_bbox(box2)\n",
    "        b1_x1, b1_x2, b1_y1, b1_y2 = b1[:, 0], b1[:, 1], b1[:, 2], b1[:, 3]\n",
    "        b2_x1, b2_x2, b2_y1, b2_y2 = b2[:, 0], b2[:, 1], b2[:, 2], b2[:, 3]\n",
    "\n",
    "        intersect_x1 = torch.max(b1_x1, b2_x1)\n",
    "        intersect_y1 = torch.max(b1_y1, b2_y1)\n",
    "        intersect_x2 = torch.min(b1_x2, b2_x2)\n",
    "        intersect_y2 = torch.min(b1_y2, b2_y2)\n",
    "\n",
    "        intersect_area = (intersect_x2 - intersect_x1 + 1) * (intersect_y2 - intersect_y1 + 1)\n",
    "\n",
    "        # union area\n",
    "        b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "        b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "        iou = intersect_area / (b1_area + b2_area - intersect_area + 1e-16)\n",
    "        return iou\n",
    "\n",
    "    def _get_anchor_boxes(self, g_i, g_j, grid_size, center_x=0.5, center_y=0.5):\n",
    "        \"\"\"\n",
    "        Creates list of anchor boxes with given dimensions (height, width) w.r.t image size\n",
    "        anchor box =  [center_x, center_y, width, height] w.r.t. the whole image\n",
    "\n",
    "        :param center_x: x coordinate of the center of the anchor box w.r.t. cell\n",
    "        :param center_y: y coordinate of the center of the anchor box w.r.t. cell\n",
    "        :return: (tensor) torch.size(len(of anchor_aspect_ratios), 4)\n",
    "        \"\"\"\n",
    "\n",
    "        anchors = []\n",
    "        center_x = (center_x + g_i) / grid_size\n",
    "        center_y = (center_y + g_j) / grid_size\n",
    "        for dims in self.anchors_dims:\n",
    "            anchors.append([center_x, center_y, dims[0], dims[1]])\n",
    "        return torch.tensor(anchors).to(self.device)\n",
    "\n",
    "    def _format_bbox(self, box):\n",
    "        \"\"\"\n",
    "        Convert [[c_x, c_y, w, h], ...] to [[x_top_left_0, y_top_left_0, x_bottom_right_0, y_bottom_right_0], ...]\n",
    "        :param box: (torch.tensor) [[c_x, c_y, w, h], ...]\n",
    "        :return: (torch.tensor) [[x_top_left_0, y_top_left_0, x_bottom_right_0, y_bottom_right_0], ...]\n",
    "        \"\"\"\n",
    "\n",
    "        x1, x2 = (box[:, 0] - box[:, 2] / 2).unsqueeze(0), (\n",
    "                box[:, 0] + box[:, 2] / 2).unsqueeze(0)\n",
    "        y1, y2 = (box[:, 1] - box[:, 3] / 2).unsqueeze(0), (\n",
    "                box[:, 1] + box[:, 3] / 2).unsqueeze(0)\n",
    "        return torch.cat((torch.t(x1 * self.image_size), torch.t(x2 * self.image_size), torch.t(y1 * self.image_size),\n",
    "                          torch.t(y2 * self.image_size)), 1)\n",
    "\n",
    "    def non_maximum_suppression(self, bbox_prediction, landmarks_prediction, conf_thresh=0.5, iou_thresh=0.5):\n",
    "        batch_size, grid_size, num_anchors = bbox_prediction.size(0), bbox_prediction.size(1), bbox_prediction.size(3)\n",
    "\n",
    "        bbox_prediction = bbox_prediction.view(batch_size, grid_size * grid_size * num_anchors, 4 + 1)\n",
    "        bbox_prediction = torch.cat((torch.expm1(bbox_prediction[:, :, :4]), bbox_prediction[:, :, 4:]), dim=2)\n",
    "\n",
    "        landmarks_prediction = landmarks_prediction.view(batch_size, grid_size * grid_size * num_anchors, 68 * 2)\n",
    "\n",
    "        conf_mask = (bbox_prediction[:, :, 4] > conf_thresh).float().unsqueeze(2)\n",
    "        bbox_pred = bbox_prediction * conf_mask\n",
    "        landmarks_pred = landmarks_prediction * conf_mask\n",
    "\n",
    "        bbox = []\n",
    "        landmarks = []\n",
    "        for i in range(batch_size):\n",
    "            image_bbox = bbox_pred[i]\n",
    "            image_landmarks = landmarks_pred[i]\n",
    "            max_conf_idx = torch.argmax(image_bbox[:, 4])\n",
    "            ious = self._get_iou(image_bbox[max_conf_idx].unsqueeze(0), image_bbox)\n",
    "            mask = ious < iou_thresh\n",
    "            bbox.append(image_bbox[mask.byte()])\n",
    "            bbox.append(image_bbox[max_conf_idx].unsqueeze(0))\n",
    "            landmarks.append(image_landmarks[mask.byte()])\n",
    "            landmarks.append(image_landmarks[max_conf_idx].unsqueeze(0))\n",
    "        bbox = torch.cat(bbox, dim=0)\n",
    "        bbox = torch.cat((self._format_bbox(bbox), bbox[:, 4:]), dim=1)\n",
    "        landmarks = torch.cat(landmarks, dim=0)\n",
    "        return bbox, landmarks.view(-1, 68, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSSD(nn.Module):\n",
    "    def __init__(self, num_classes=1, num_points=68, width_mult=1.0, use_batch_norm=True):\n",
    "        super(CustomSSD, self).__init__()\n",
    "        self.base = MobileNetV2().features\n",
    "        self.source_layer_indexes = [GraphPath(14, 'conv', 3),19,]\n",
    "        \n",
    "        #change convolutions for inverted residual to change number of operations \n",
    "        self.extras = ModuleList([\n",
    "            InvertedResidual(1280, 512, stride=2, expand_ratio=0.2),\n",
    "            InvertedResidual(512, 256, stride=2, expand_ratio=0.25),\n",
    "            InvertedResidual(256, 256, stride=2, expand_ratio=0.5),\n",
    "            Conv2d(256, 64, kernel_size=2, stride=1, padding=0)\n",
    "        ])\n",
    "        \n",
    "        self.regression_headers = ModuleList([\n",
    "            SeperableConv2d(in_channels=round(576 * width_mult), out_channels=6 * 4,\n",
    "                            kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=1280, out_channels=6 * 4, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=512, out_channels=6 * 4, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=256, out_channels=6 * 4, kernel_size=3, padding=1),\n",
    "            Conv2d(in_channels=64, out_channels=6 * 4, kernel_size=1),\n",
    "        ])\n",
    "        \n",
    "        self.classification_headers = ModuleList([\n",
    "            SeperableConv2d(in_channels=round(576 * width_mult), out_channels=6 * num_classes, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=1280, out_channels=6 * num_classes, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=512, out_channels=6 * num_classes, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=256, out_channels=6 * num_classes, kernel_size=3, padding=1),\n",
    "            Conv2d(in_channels=64, out_channels=6 * num_classes, kernel_size=1),\n",
    "        ])\n",
    "        \n",
    "        self.landmarks_headers = ModuleList([\n",
    "            SeperableConv2d(in_channels=round(576 * width_mult), out_channels=6 * num_points * 2, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=1280, out_channels=6 * num_points * 2, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=512, out_channels=6 * num_points * 2, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=256, out_channels=6 * num_points * 2, kernel_size=3, padding=1),\n",
    "            SeperableConv2d(in_channels=256, out_channels=6 * num_points * 2, kernel_size=3, padding=1),\n",
    "            Conv2d(in_channels=64, out_channels=6 * num_points * 2, kernel_size=1),\n",
    "        ])\n",
    "        \n",
    "        self.source_layer_add_ons = nn.ModuleList([t[1] for t in self.source_layer_indexes\n",
    "                                                   if isinstance(t, tuple) and not isinstance(t, GraphPath)])\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_points = num_points\n",
    "        \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        confidences = []\n",
    "        locations = []\n",
    "        landmarks = []\n",
    "        start_layer_index = 0\n",
    "        header_index = 0\n",
    "        for end_layer_index in self.source_layer_indexes:\n",
    "            if isinstance(end_layer_index, GraphPath):\n",
    "                path = end_layer_index\n",
    "                end_layer_index = end_layer_index.s0\n",
    "                added_layer = None\n",
    "            elif isinstance(end_layer_index, tuple):\n",
    "                added_layer = end_layer_index[1]\n",
    "                end_layer_index = end_layer_index[0]\n",
    "                path = None\n",
    "            else:\n",
    "                added_layer = None\n",
    "                path = None\n",
    "            for layer in self.base[start_layer_index: end_layer_index]:\n",
    "                x = layer(x)\n",
    "            if added_layer:\n",
    "                y = added_layer(x)\n",
    "            else:\n",
    "                y = x\n",
    "            if path:\n",
    "                sub = getattr(self.base[end_layer_index], path.name)\n",
    "                for layer in sub[:path.s1]:\n",
    "                    x = layer(x)\n",
    "                y = x\n",
    "                for layer in sub[path.s1:]:\n",
    "                    x = layer(x)\n",
    "                end_layer_index += 1\n",
    "            start_layer_index = end_layer_index\n",
    "            confidence, location, landmark = self.compute_header(header_index, y)\n",
    "            header_index += 1\n",
    "            confidences.append(confidence)\n",
    "            locations.append(location)\n",
    "            landmarks.append(landmark)\n",
    "\n",
    "        for layer in self.base[end_layer_index:]:\n",
    "            x = layer(x)\n",
    "\n",
    "        for layer in self.extras:\n",
    "            x = layer(x)\n",
    "            confidence, location, landmark = self.compute_header(header_index, x)\n",
    "            header_index += 1\n",
    "            confidences.append(confidence)\n",
    "            locations.append(location)\n",
    "            landmarks.append(landmark)\n",
    "\n",
    "        confidences = torch.cat(confidences, 1)\n",
    "        locations = torch.cat(locations, 1)\n",
    "        landmarks = torch.cat(landmarks, 1)\n",
    "        return confidences, locations, landmarks\n",
    "\n",
    "    def compute_header(self, i, x):\n",
    "        confidence = self.classification_headers[i](x)\n",
    "        confidence = confidence.permute(0, 2, 3, 1).contiguous()\n",
    "        confidence = confidence.view(confidence.size(0), -1, self.num_classes)\n",
    "\n",
    "        location = self.regression_headers[i](x)\n",
    "        location = location.permute(0, 2, 3, 1).contiguous()\n",
    "        location = location.view(location.size(0), -1, 4)\n",
    "        \n",
    "        landmarks = self.landmarks_headers[i](x)\n",
    "        landmarks = landmarks.permute(0, 2, 3, 1).contiguous()\n",
    "        landmarks = landmarks.view(location.size(0), -1, self.num_points * 2)\n",
    "\n",
    "        return confidence, location, landmarks\n",
    "\n",
    "    def load(self, model):\n",
    "        self.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    def save(self, model_path):\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms, utils\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def read_and_resize(filename):\n",
    "    img = Image.open(filename)\n",
    "    img = img.resize((300, 300))\n",
    "    transform = transforms.Compose([            \n",
    "     transforms.Resize(300),                    \n",
    "     transforms.ToTensor(),   \n",
    "     transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ])\n",
    "    img_t = transform(img)\n",
    "    return img_t\n",
    "\n",
    "def get_bbox(landmarks):\n",
    "    \"\"\"\n",
    "    Finds bounding box coordinates in format [xmin, ymin, width, height] from landmarks\n",
    "    :param landmarks: torch.size(68,2)\n",
    "    :return: torch.size(4)\n",
    "    \"\"\"\n",
    "\n",
    "    x = landmarks[:, 0]\n",
    "    y = landmarks[:, 1]\n",
    "\n",
    "    xmin, xmax = min(x), max(x)\n",
    "    ymin, ymax = min(y), max(y)\n",
    "\n",
    "    width = xmax - xmin\n",
    "    height = ymax - ymin\n",
    "    return [xmin, ymin, width, height]\n",
    "\n",
    "class FaceLandmarksDataset(Dataset):\n",
    "    \"\"\"Face dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.dataset['file_name'][idx]\n",
    "        image = read_and_resize(image_name)\n",
    "        coordinates_int = []\n",
    "        for i in range(0, len(self.dataset.iloc[idx][1:])):\n",
    "            if i % 2 == 0:\n",
    "                (x, y) = ((int) (self.dataset.iloc[idx][1:][i]), (int) (self.dataset.iloc[idx][1:][i + 1]))\n",
    "                coordinates_int.append((x,y))\n",
    "        \n",
    "        xmin, ymin, width, height = get_bbox(landmarks = torch.tensor(coordinates_int))\n",
    "                                       \n",
    "        normalized_points = []\n",
    "        for (x,y) in coordinates_int:\n",
    "            normalized_points.append(((x - xmin.double())/width.double(), (y - ymin.double())/height.double()))\n",
    "       \n",
    "        return (image, [(torch.tensor([xmin, ymin, width, height]), torch.tensor(normalized_points))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(1):\n",
    "#     batch = next(iter(train_loader))\n",
    "#     print(len(batch))\n",
    "#     print(batch[0].shape)   #INPUT image\n",
    "#     print(batch[1][0][0].shape) # Boxes \n",
    "#     print(batch[1][0][1].shape) # Coordintes of landmarks\n",
    "#     condifence, locations,landmarks = net(batch[0])\n",
    "\n",
    "# images, boxes, labels = data\n",
    "# images = images.to(device)\n",
    "# boxes = boxes.to(device)\n",
    "# labels = labels.to(device)\n",
    "def train(loader, net, criterion, optimizer, device, debug_steps=100, epoch=-1):\n",
    "    net.train(True)\n",
    "    running_loss = 0.0\n",
    "    running_regression_loss = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "            \n",
    "    for i, data in enumerate(loader):\n",
    "        images, gt_boxes = data\n",
    "        print(len(gt_boxes), type(gt_boxes[0]), \" Boxes len\")\n",
    "        images = images.to(device)\n",
    "        gt_locations = gt_boxes[0][0].to(device)\n",
    "        gt_landmarks = gt_boxes[0][1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        confidence, locations, landmarks_prediction = net(images)\n",
    "        \"\"\"\n",
    "            Joint loss for landmarks(NME) and bounding boxes(localization loss, confidence loss)\n",
    "            :param prediction: (tuple) containing predictions for bounding boxes and landmarks\n",
    "            bounding boxes prediction:  torch.size(batch_size, grid_size, grid_size, num_anchor_box, 4+1)\n",
    "            landmarks prediction: torch.size(batch_size, grid_size, grid_size, num_anchor_box, 68, 2 or 3)\n",
    "\n",
    "            :param target: (tuple) containing target values  for bounding boxes and landmarks\n",
    "            bounding boxes targets: torch.size(batch_size, max_num_of_faces, 4+1)\n",
    "            landmarks targets: torch.size(batch_size, max_num_of_faces, 68, 2 or 3)\n",
    "\n",
    "            :return: normalized mean error(float), localization loss(float), confidence loss(float),\n",
    "            best predicted bbox matched with gt (torch.tensor)\n",
    "        \"\"\"\n",
    "        \n",
    "        landmarks_prediction = landmarks_prediction.resize_((1,len(landmarks_prediction[0]),68,2))\n",
    "        print(landmarks_prediction.shape, \" landmarks prediction: torch.size(batch_size, grid_size, grid_size, num_anchor_box, 68, 2 or 3)\")\n",
    "        print(gt_landmarks.shape, \" landmarks target SHAPE must be torch.size(batch_size, max_num_of_faces, 68, 2 or 3)\")\n",
    "        print()\n",
    "        bounding_boxes_prediction = torch.cat( (locations, condifence), dim=2)\n",
    "        \n",
    "        prediction = (bounding_boxes_prediction, landmarks_prediction)\n",
    "        target = (gt_locations, gt_landmarks)\n",
    "        #TODO convert locations to bounding box predictions\n",
    "        nme, loc_loss, conf_loss, bbox_confirmed, landmarks_confirmed = criterion( prediction,  )  # TODO CHANGE BOXES\n",
    "        #Output from joint loss format\n",
    "        #nme, loc_loss, conf_loss, self.non_maximum_suppression(bbox_prediction, landmarks_prediction)\n",
    "        loss = nme + loc_loss + conf_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_nme_loss += nme.item()\n",
    "        running_loc_loss += loc_loss.item()\n",
    "        running_conf_loc_loss += conf_loss.item()\n",
    "        if i and i % debug_steps == 0:\n",
    "            avg_loss = running_loss / debug_steps\n",
    "            avg_nme_loss = running_nme_loss / debug_steps\n",
    "            avg_loc_loss = running_loc_loss / debug_steps\n",
    "            avg_conf_loc_loss = running_conf_loc_loss / debug_steps\n",
    "            logging.info(\n",
    "                f\"Epoch: {epoch}, Step: {i}, \" +\n",
    "                f\"Average Loss: {avg_loss:.4f}, \" +\n",
    "                f\"Average nme Loss {avg_nme_loss:.4f}, \" +\n",
    "                f\"Average loc Loss: {avg_loc_loss:.4f}\" +\n",
    "                f\"Average conf Loss: {avg_conf_loc_loss:.4f}\"\n",
    "            )\n",
    "            running_loss = 0\n",
    "            running_nme_loss = 0\n",
    "            running_loc_loss = 0\n",
    "            running_conf_loc_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, net, criterion, device):\n",
    "    net.eval()\n",
    "    running_loss = 0.0\n",
    "    running_regression_loss = 0.0\n",
    "    running_classification_loss = 0.0\n",
    "    num = 0\n",
    "    for _, data in enumerate(loader):\n",
    "        images, boxes, labels = data\n",
    "        images = images.to(device)\n",
    "        boxes = boxes.to(device)\n",
    "        labels = labels.to(device)\n",
    "        num += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            confidence, locations = net(images)\n",
    "            regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)\n",
    "            loss = regression_loss + classification_loss\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_regression_loss += regression_loss.item()\n",
    "        running_classification_loss += classification_loss.item()\n",
    "    return running_loss / num, running_regression_loss / num, running_classification_loss / num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-17 21:45:48,419 - root - INFO - Use Cuda.\n"
     ]
    }
   ],
   "source": [
    "scheduler = \"cosine\"\n",
    "lr = 0.01\n",
    "t_max = 200 # Params for Cosine Annealing\n",
    "\n",
    "mb2_width_mult = 1.0 #Width Multiplifier for MobilenetV2\n",
    "# Params for SGD\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "gamma = 0.1\n",
    "\n",
    "# Params for Multi-step Scheduler\n",
    "milestones = \"80,100\" #milestones for MultiStepLR\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "num_workers = 0\n",
    "validation_epochs = 0 \n",
    "debug_steps = 100 #Set the debug log output frequency\n",
    "\n",
    "# Train params\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    logging.info(\"Use Cuda.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphPath = namedtuple(\"GraphPath\", ['s0', 'name', 's1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-17 21:45:48,730 - root - INFO - Prepare training datasets.\n",
      "2019-11-17 21:45:48,820 - root - INFO - Train dataset size: 3449\n"
     ]
    }
   ],
   "source": [
    "config = mobilenetv1_ssd_config\n",
    "\n",
    "logging.info(\"Prepare training datasets.\")\n",
    "\n",
    "# PREPARE DATASET\n",
    "batch_size = 1\n",
    "test_labels = pd.read_csv('./LS3D-W/test.csv')\n",
    "train_dataset = FaceLandmarksDataset(test_labels)\n",
    "train_loader = DataLoader (train_dataset, batch_size = batch_size, shuffle = True, pin_memory = True)\n",
    "logging.info(\"Train dataset size: {}\".format(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network created\n"
     ]
    }
   ],
   "source": [
    "net = CustomSSD()\n",
    "min_loss = -10000.0\n",
    "last_epoch = -1\n",
    "#net.init_from_base_net(base_net)\n",
    "net.to(DEVICE)\n",
    "print(\"Network created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1, 3, 300, 300])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 68, 2])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    batch = next(iter(train_loader))\n",
    "    print(len(batch))\n",
    "    print(batch[0].shape)   #INPUT image\n",
    "    print(batch[1][0][0].shape) # Boxes \n",
    "    print(batch[1][0][1].shape) # Coordintes of landmarks\n",
    "    condifence, locations,landmarks = net(batch[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-17 21:45:50,286 - root - INFO - Uses CosineAnnealingLR scheduler.\n",
      "2019-11-17 21:45:50,288 - root - INFO - Start training from epoch 0.\n",
      "1 <class 'list'>  Boxes len\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot resize variables that require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-239-9f5263a7ed85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     train(train_loader, net, criterion, optimizer,\n\u001b[1;32m---> 28\u001b[1;33m           device=DEVICE, debug_steps=debug_steps, epoch=epoch)\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalidation_epochs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-232-880173760366>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(loader, net, criterion, optimizer, device, debug_steps, epoch)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \"\"\"\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mlandmarks_prediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlandmarks_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlandmarks_prediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m68\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlandmarks_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" landmarks prediction: torch.size(batch_size, grid_size, grid_size, num_anchor_box, 68, 2 or 3)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt_landmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" landmarks target SHAPE must be torch.size(batch_size, max_num_of_faces, 68, 2 or 3)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cannot resize variables that require grad"
     ]
    }
   ],
   "source": [
    "#Заменить на своё\n",
    "criterion = JointLoss(device = DEVICE)\n",
    "\n",
    "params = [\n",
    "    {'params': net.base.parameters(), 'lr': lr},\n",
    "    {'params': itertools.chain(\n",
    "        net.source_layer_add_ons.parameters(),\n",
    "        net.extras.parameters()\n",
    "    ), 'lr': lr},\n",
    "    {'params': itertools.chain(\n",
    "        net.regression_headers.parameters(),\n",
    "        net.classification_headers.parameters(),\n",
    "        net.landmarks_headers.parameters()\n",
    "    )}\n",
    "]   \n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(params, lr = lr, momentum = momentum,\n",
    "                            weight_decay = weight_decay)\n",
    "\n",
    "#args.scheduler == 'cosine':\n",
    "logging.info(\"Uses CosineAnnealingLR scheduler.\")\n",
    "scheduler = CosineAnnealingLR(optimizer, t_max, last_epoch=last_epoch)\n",
    "\n",
    "logging.info(f\"Start training from epoch {last_epoch + 1}.\")\n",
    "for epoch in range(last_epoch + 1, num_epochs):\n",
    "    scheduler.step()\n",
    "    train(train_loader, net, criterion, optimizer,\n",
    "          device=DEVICE, debug_steps=debug_steps, epoch=epoch)\n",
    "\n",
    "    if epoch % validation_epochs == 0 or epoch == num_epochs - 1:\n",
    "        val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)\n",
    "        logging.info(\n",
    "            f\"Epoch: {epoch}, \" +\n",
    "            f\"Validation Loss: {val_loss:.4f}, \" +\n",
    "            f\"Validation Regression Loss {val_regression_loss:.4f}, \" +\n",
    "            f\"Validation Classification Loss: {val_classification_loss:.4f}\"\n",
    "        )\n",
    "        model_path = os.path.join(checkpoint_folder, f\"{net}-Epoch-{epoch}-Loss-{val_loss}.pth\")\n",
    "        net.save(model_path)\n",
    "        logging.info(f\"Saved model {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
